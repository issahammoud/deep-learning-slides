{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport glob\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"train_male = len(os.listdir(\"/kaggle/input/gender-classification-dataset/Training/male\"))\ntrain_female = len(os.listdir(\"/kaggle/input/gender-classification-dataset/Training/female\"))\nvalid_male = len(os.listdir(\"/kaggle/input/gender-classification-dataset/Validation/male\"))\nvalid_female = len(os.listdir(\"/kaggle/input/gender-classification-dataset/Validation/female\"))\nprint(f\"There are {train_male} male and {train_female} female in the training set\")\nprint(f\"There are {valid_male} male and {valid_female} female in the validation set\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shapes = np.array([cv2.imread(path, -1).shape for path in np.random.choice(glob.glob(\"/kaggle/input/gender-classification-dataset/**/**/*\"), 1000, replace=False)])\nif not np.all(shapes==shapes[0]):\n    print(\"Not all images have the same shape\")\nelse:\n    print(f\"Almost all images are of shape {shapes[0]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.min(shapes, axis=0), np.max(shapes, axis=0), np.mean(shapes, axis=0), np.median(shapes, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"median_shape = np.median(shapes, axis=0).astype(int)\nmin_shape = np.min(shapes, axis=0).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_imgs = np.random.choice(glob.glob(\"/kaggle/input/gender-classification-dataset/**/**/*\"), 30, replace=False)\nplt.figure(figsize = (18, 5))\nplt.suptitle(f\"Example of {len(random_imgs)} random images from all the data\")\nfor i in range(len(random_imgs)):\n    plt.subplot(3,10,i+1)\n    img = cv2.imread(random_imgs[i], -1)[...,::-1]/255.\n    noise = np.random.normal(0, 0.2, size=img.shape)\n    img = img + noise\n    img = np.clip(img, 0, 1)\n    plt.imshow(img)\n    plt.axis('off')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loader","metadata":{}},{"cell_type":"code","source":"female_path = glob.glob(\"/kaggle/input/gender-classification-dataset/Training/female/*\")\nfemale_label = [1] * len(female_path)\nmale_path = glob.glob(\"/kaggle/input/gender-classification-dataset/Training/male/*\")\nmale_label = [0] * len(male_path)\nall_pathes = female_path + male_path\nall_labels = female_label + male_label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will create a class to load and generate data to our model. The class will be responsible of reading, shuffling and feeding the data into the fully connected network. ","metadata":{}},{"cell_type":"code","source":"class DataLoader:\n    def __init__(self, data_path, batch_size, shape):\n        self._X = self._read_data_path(data_path)\n        self._batch_size = batch_size\n        self.nb_iterations = self.__len__()//batch_size\n        self._shape = shape\n    \n    def __len__(self):\n        return len(self._X)\n    \n    def get_shape(self):\n        return self._shape + (3, )\n    \n    def _read_data_path(self, data_path):\n        \"\"\"\n        This method takes the path to training or validation data,\n        and return two arrays X and Y. X contains the full path to\n        all images, and Y contains the corresponding labels:\n        0 for male, 1 for female.\n        \n        data_path: str, the path to the training or validation dataset\n        return:\n        tuple (X, Y) of type np.array each\n        \"\"\"\n        female_path = glob.glob(f\"{data_path}/female/*\")\n        male_path = glob.glob(f\"{data_path}/male/*\")\n        all_pathes = female_path + male_path\n        return all_pathes\n    \n\n    def _read_single_image(self, img_path):\n        \"\"\"\n        This method takes an image path and a label, read the image and retrun it with the label.\n        The image should be converted into gray scale, resized into self._shape,\n        normalized to 1, and vectorized.\n        \"\"\"\n        img = tf.io.decode_png(\n            tf.io.read_file(img_path), channels=3, dtype=tf.uint8\n        )\n        resized_img = tf.image.resize(img, self._shape) / 255\n        noise = tf.random.normal(shape=self._shape + (3,), mean=0.0, stddev=0.2)\n        \n        return tf.clip_by_value(resized_img + noise, 0, 1), resized_img\n    \n    def get_dataset(self):\n        \"\"\"\n        This method should create a dataset from the image path and label,\n        shuffle them, repeat the dataset, read the actual images,\n        create batches and return a dataset to be fed to the model. \n        \"\"\"\n        dataset = tf.data.Dataset.from_tensor_slices(self._X)\n        dataset = dataset.shuffle(buffer_size=self.__len__(), reshuffle_each_iteration=True).repeat()\n        dataset = dataset.map(self._read_single_image, tf.data.AUTOTUNE)\n        dataset = dataset.batch(self._batch_size, num_parallel_calls=tf.data.AUTOTUNE)\n        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n        return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Network","metadata":{}},{"cell_type":"code","source":"def conv_bn_relu(x, feature_maps, strides, kernel_size):\n    x = tf.keras.layers.Conv2D(\n          filters=feature_maps, kernel_size=kernel_size,\n          strides=strides, padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    return x\n\n\ndef encoder_residual_block(x, feature_maps_list, kernel_size_list, drop_rate, downsample=True):\n    x = conv_bn_relu(x, feature_maps_list[-1], strides=2 if downsample else 1, kernel_size=1)\n    residual = x\n    \n    if drop_rate > 0:\n        x = tf.keras.layers.SpatialDropout2D(drop_rate)(x)\n\n    for i in range(len(feature_maps_list)):\n        x = conv_bn_relu(x, feature_maps_list[i], strides=1, kernel_size=kernel_size_list[i])\n\n    return tf.keras.layers.Add()([x, residual])\n\n\ndef decoder_residual_block(x, encoder_block, feature_maps_list, kernel_size_list, drop_rate):\n    x = tf.keras.layers.UpSampling2D()(x)\n    if x.shape[1:-1] != encoder_block.shape[1:-1]:\n        x = tf.image.resize_with_crop_or_pad(x, encoder_block.shape[1], encoder_block.shape[2])\n    \n    residual = x\n    \n    if x.shape[-1] != feature_maps_list[-1]:\n        residual = conv_bn_relu(x, feature_maps_list[-1], strides=1, kernel_size=1)\n\n    if drop_rate > 0:\n        x = tf.keras.layers.SpatialDropout2D(drop_rate)(x)\n\n    for i in range(len(feature_maps_list)):\n        x = conv_bn_relu(x, feature_maps_list[i], strides=1, kernel_size=kernel_size_list[i])\n\n    return tf.keras.layers.Add()([x, residual])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_encoder_decoder_model(input_shape, config):\n    input_image = tf.keras.layers.Input(input_shape)\n\n    encoded = input_image\n    encoder_layers = []\n    for i in range(len(config[\"encoder_fmaps\"])):\n        encoded = encoder_residual_block(encoded, config[\"encoder_fmaps\"][i], config[\"encoder_kernels\"][i], config[\"drop_rate\"], downsample=(i!=0))\n        encoder_layers.append(encoded)\n\n\n    code = conv_bn_relu(encoded, config[\"code_fmaps\"], config[\"code_stride\"], config[\"code_kernel\"])\n\n    decoded = code\n    decoder_layers = []\n    for i in range(len(config[\"decoder_fmaps\"])):\n        decoded = decoder_residual_block(decoded, encoder_layers[-i-1], config[\"decoder_fmaps\"][i], config[\"decoder_kernels\"][i], config[\"drop_rate\"])\n        decoder_layers.append(decoded)\n\n    output = tf.keras.layers.Conv2D(filters=3, kernel_size=3, strides=1, padding='same')(decoded)\n\n    model = tf.keras.Model(input_image, output)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimizer","metadata":{}},{"cell_type":"code","source":"def create_optimizer(lr, nb_iterations, use_cosine_decay=False):\n    \"\"\"\n    This function should create an Adam optimizer.\n    If use_cosine_decay is True, it should apply a cosine_decay_with restart\n    (look at tf.keras.optimizers.schedules.CosineDecayRestarts)\n    with a cycle of 10 epochs, an alpha=0.01 and t_mul=2.0.\n    This function should return an optimizer instance.\n    \"\"\"\n    if use_cosine_decay:\n        lr = tf.keras.optimizers.schedules.CosineDecayRestarts( lr, 10*nb_iterations, t_mul=1.0, m_mul=1.0, alpha=0.01,)\n    optimizer = tf.keras.optimizers.Adam(lr)\n    return optimizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=32\nresize_shape=(120, 120)\ntraining_path=\"/kaggle/input/gender-classification-dataset/Training\"\nvalidation_path=\"/kaggle/input/gender-classification-dataset/Validation\"\n\ntraining_data = DataLoader(training_path, batch_size, resize_shape)\nvalidation_data = DataLoader(validation_path, batch_size, resize_shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config={\n    \"encoder_fmaps\": [[8, 8], [16, 16], [32, 32], [64, 64], [128, 128]],\n    \"encoder_kernels\": [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3]],\n\n    \"decoder_fmaps\": [[128, 128], [64, 64], [32, 32], [16, 16], [8, 8]],\n    \"decoder_kernels\": [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3]],\n    \n    \"code_fmaps\": 256,\n    \"code_stride\": 2,\n    \"code_kernel\": 5,\n\n    \"drop_rate\": 0}\n\nmodel = create_encoder_decoder_model(training_data.get_shape(), config)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate=0.01\noptimizer = create_optimizer(learning_rate, training_data.nb_iterations, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def psnr(y_true, y_pred):\n    return tf.image.psnr(y_true, y_pred, max_val=1.0)\n\ndef ssim(y_true, y_pred):\n    return tf.image.ssim(y_true, y_pred, max_val=1.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs=50\n\nmodel.compile(optimizer=optimizer, loss=\"mse\", metrics=['mae', psnr, ssim])\n\nhistory = model.fit(training_data.get_dataset(),\n                    validation_data=validation_data.get_dataset(),\n                    epochs=epochs, steps_per_epoch=training_data.nb_iterations, validation_steps=validation_data.nb_iterations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss = history.history['loss']\ntrain_acc = history.history['accuracy']\nvalid_loss = history.history['val_loss']\nvalid_acc = history.history['val_accuracy']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12, 6))\nplt.suptitle(\"Learning Curves\")\n    \nplt.subplot(121)\nplt.title(\"cross entropy\")\nplt.plot(np.arange(1, len(train_loss)+1), train_loss, label='training', c='b')\nplt.plot(np.arange(1, len(valid_loss)+1), valid_loss, label='validation', c='r')\nplt.xlim(1, epochs + epochs//10)\nplt.xticks(np.arange(0, epochs + epochs//10, epochs//10))\nhandles, labels = plt.gca().get_legend_handles_labels()\nby_label = dict(zip(labels, handles))\nplt.legend(by_label.values(), by_label.keys(), loc='upper right')\n\nplt.subplot(122)\nplt.title(\"accuracy\")\nplt.plot(np.arange(1, len(train_acc)+1), train_acc, label='training', c='b')\nplt.plot(np.arange(1, len(valid_acc)+1), valid_acc, label='validation', c='r')\nplt.xlim(1, epochs + epochs//10)\nplt.xticks(np.arange(0, epochs + epochs//10, epochs//10))\nhandles, labels = plt.gca().get_legend_handles_labels()\nby_label = dict(zip(labels, handles))\nplt.legend(by_label.values(), by_label.keys(), loc='lower right')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test the model on new images","metadata":{}},{"cell_type":"code","source":"fake_faces = []\nfor img_path in glob.glob(\"/kaggle/input/fakefaces/*\"):\n    img = cv2.cvtColor(cv2.imread(img_path, -1), cv2.COLOR_BGR2RGB)/255.\n    noise = np.random.normal(0, 0.3, size=img.shape)\n    img = img + noise\n    img = np.clip(img, 0, 1)\n    fake_faces.append(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(24, 5))\nfor i in range(len(fake_faces)):\n    plt.subplot(1, len(fake_faces), i+1)\n    plt.imshow(fake_faces[i])\n    plt.axis('off')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_faces = []\nfor img in fake_faces:\n    \n    img = cv2.resize(img, resize_shape)\n    batch_faces.append(img)\n    \nbatch_faces = np.array(batch_faces)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model(batch_faces)\nplt.figure(figsize=(24, 5))\nfor i in range(len(fake_faces)):\n    plt.subplot(1, len(fake_faces), i+1)\n    plt.imshow(preds[i])\n    plt.axis('off')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}